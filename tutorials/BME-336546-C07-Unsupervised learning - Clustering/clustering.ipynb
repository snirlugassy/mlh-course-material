{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME-336546-C07-Unsupervised learning - Clustering (k-means and GMM)\n",
    "Start by updating your venv with `tutorial8.yml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main ML topic:\n",
    "Unsuprvised learning. Our data is not labeld so we try to cluster our examples according to their features distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our mission\n",
    "Clustering our data \"correctly\" with no prior knowladge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory reminders\n",
    "### k-means\n",
    "*k-means* is a method that tries to estimate the centroids of the unknown $K$ clusters when $K$ is given to us. This algorithm is an iterative algorithm that starts with random centroids and updates them by the assignment of the data to the cluster. The data points are assigned to a cluster by searching the minimal \"distance\" between each data point to the centroid using some distance measure $d(x,\\mu)$. \n",
    "\n",
    "\n",
    "The formalized algorithm is as follows:\n",
    "1. Set the number of clusters ($K$), number of iterations ($N$) and a distance measure $d(\\bullet,\\bullet)$.\n",
    "2. Scale your data $D=\\{x^{(1)},\\dots,x^{(m)}\\} \\in \\mathbb{R}^n$.\n",
    "3. Intialize $\\{\\mu_1,\\dots,\\mu_K\\}$.\n",
    "4. $ \\forall i, C_i = \\{x \\in D, i=\\underset{j}{argmin} \\space d(x,\\mu_j)\\} $\n",
    "5. $ \\forall i, \\mu_i= \\underset{\\mu}{argmin}\\sum_{x\\in C_i}{}d(x,\\mu) $\n",
    "6. Repeat (4)-(5) $N$ times.\n",
    "\n",
    "\n",
    "Note the following:\n",
    "* k-means is sensitive to scaling.\n",
    "* It is highly depended on intializations and the number $K$.\n",
    "* A different distance measure would result in different clustering.\n",
    "* This is a \"hard clustering\" method, meaning a data point either belongs to a cluster $C_i$ or it does not. \n",
    "\n",
    "We can visualize this process [here](https://stanford.edu/class/engr108/visualizations/kmeans/kmeans.html).\n",
    "\n",
    "### Gaussian mixture models\n",
    "In Gaussian mixture models (GMM), we assume that our data was drawn from different clusters where each of them is Gaussian distributed. Our aim is actually to estimate the deterministic parameters of theses distributions (expected values, $\\mu_z$, and covariance matrices, $\\Sigma_z$) and cluster probabilities, $\\pi_z$. In order to do so, we would use the maximum likelihood estimation (MLE). Thus, We would like to maximize the likelihood term:\n",
    "$$\\begin{equation}\n",
    "L(\\pi,\\mu,\\Sigma)=\\prod_{i=1}^mp(x^{(i)})=\\prod_{i=1}^m\\sum_{z=1}^{K}\\mathcal{N}(x^{(i)}|\\mu_z,\\Sigma_z)\\pi_z\n",
    "\\end{equation}$$\n",
    "We maximize this term by using an iterative algorithm that updates the initialized parameters that in their turn, update the *responsibility term* ($\\gamma$) which in its' turn updates the parameters and so on and so forth. The mentioned terms are calculated as follows:\n",
    "$$\\begin{equation}\n",
    "\\gamma_i^c=\\frac{\\mathcal{N}(x^{(i)}|\\mu_c,\\Sigma_c)\\pi_c}{\\sum_{z=1}^K\\mathcal{N}(x^{(i)}|\\mu_z,\\Sigma_z)\\pi_z}\n",
    "\\tag{1}\n",
    "\\end{equation}$$\n",
    "$$\\begin{equation}\n",
    "m_c=\\sum_{i=1}^{m}\\gamma_i^c\n",
    "\\tag{2}\n",
    "\\end{equation}$$\n",
    "$$\\begin{equation}\n",
    "\\mu_c=\\frac{1}{m_c}\\sum_{i=1}^{m}\\gamma_i^cx^{(i)}\n",
    "\\tag{3}\n",
    "\\end{equation}$$\n",
    "$$\\begin{equation}\n",
    "\\Sigma_c=\\frac{1}{m_c}\\sum_{i=1}^{m}\\gamma_i^c(x^{(i)} - \\mu_c)(x^{(i)} - \\mu_c)^T\n",
    "\\tag{4}\n",
    "\\end{equation}$$\n",
    "$$\\begin{equation}\n",
    "\\pi_c = \\frac{m_c}{m}\n",
    "\\tag{5}\n",
    "\\end{equation}$$\n",
    "The responsabilities represent the probability of an example to belong to some cluster given the values of the example's features i.e.\n",
    "$$\\begin{equation}\n",
    "\\gamma_i^c=\\overbrace{P(Z=c|X=x^{(i)})}^{A posterior}\\overset{Bayes}{=}\\frac{\\overbrace{P(X=x^{(i)}|Z=c)}^{Likelihood}\\overbrace{P(Z=c)}^{Prior}}{\\underbrace{P(X=x^{(i)})}_{Evidence}}\n",
    "\\end{equation}$$\n",
    "Thus, the vector $(\\gamma_i^1,\\dots \\gamma_i^K)$ is a *soft* assignment of $x^{(i)}$ in contrary to k-means. This means that GMM does not really assign an example to a single cluster but rather calculates the probability to belong to each cluster. In this aspect, the comparison of k-means vs. GMM is analogous to SVM vs. logistic regression respectively. \n",
    "\n",
    "We can visualize this process [here](https://lukapopijac.github.io/gaussian-mixture-model/).\n",
    "\n",
    "\n",
    "### K evaluation\n",
    "An important topic in clustering algorithms is choosing $K$. Two methods to choose $K$ that are complementary to each other are the *elbow curve* and *Silhouette Curve*. The first one computes the sum of squered errors $(SSE)$ as function of $K$ as it tries to capture how spreaded the data points are around the adequate centroids once the algorithm converged. The $K$ that creates an \"elbow\" is chosen. $SSE$ is computed as follows:\n",
    "$$\\begin{equation}\n",
    "SSE(K)=\\sum_{k=1}^{K}\\sum_{x \\in C_k}{}d(x,\\mu_k)^2\n",
    "\\end{equation}$$\n",
    "\n",
    "\n",
    "The *Silhouette coefficient* is a measure of how tightly grouped all the points in the cluster are. Thus, it is a measure of how appropriately the data have been clustered. Let us define our $i^{th}$ example of our dataset assigned to $C_k$ as $x_{ik}$.\n",
    "\n",
    "We start by estimating the mean distance of $x_{ik}$ to it's neighbors within the same cluster:\n",
    "$$\\begin{equation}\n",
    "a(x_{ik})=\\frac{1}{|C_i|-1}\\sum_{x_{jk}, i \\neq j}{}d(x_{ik},x_{jk})\n",
    "\\end{equation}$$\n",
    "We then define the mean dissimilarity of point $x_{ik}$ to some **other** cluster $C_n$ as the mean of the distance from $x_{ik}$ to all points in $C_n$. \n",
    "\n",
    "$b(x_{ik})$ is the mean distance of the \"closest\" cluster.\n",
    "$$\\begin{equation}\n",
    "b(x_{ik})=\\underset{n \\neq k}{\\min}\\frac{1}{|C_n|}\\sum_{x_{jn}}{}d(x_{ik},x_{jn})\n",
    "\\end{equation}$$\n",
    "The *Silhouette coefficient* is calculated as:\n",
    "$$\\begin{equation}\n",
    "s(x_{ik})=\\frac{b(x_{ik})-a(x_{ik})}{\\max\\{a(x_{ik}),b(x_{ik})\\}},-1\\leq s(x_{ik}) \\leq 1\n",
    "\\end{equation}$$\n",
    "The closer the coefficient is to 1, the better the assignment of the example is. The closer the coefficient is to -1, the better the assignment of the example would have been to the neighboring clusetr.\n",
    "\n",
    "The *Silhouette score* is calculated as:\n",
    "$$\\begin{equation}\n",
    "S_c(K)=\\underset{k}{\\max}\\frac{1}{|C_k|}\\sum_{x_{ik}}{}s(x_{ik})\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(['ggplot']) \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with generating our data, split it, scale it and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, true_labels = make_blobs(n_samples=1000,centers=4,cluster_std=1.75,random_state=336543)\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(features,true_labels,test_size=0.2, random_state=336546)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_orig)\n",
    "X_test = scaler.transform(X_test_orig)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "for idx, ax in enumerate(axs.flatten()):\n",
    "    if idx == 0:\n",
    "        sns.scatterplot(data=X_train, x=X_train[:,0], y=X_train[:,1],ax=ax)\n",
    "        ax.set_title('Train')\n",
    "    else:\n",
    "        sns.scatterplot(data=X_test, x=X_test[:,0], y=X_test[:,1],ax=ax)\n",
    "        ax.set_title('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This is the original data with no labels at all.\n",
    "\n",
    "<span style=\"color:red\">***Question:***</span> *How many clusters do you see?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see from how many distributions this data was drawn from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "for idx, ax in enumerate(axs.flatten()):\n",
    "    if idx == 0:\n",
    "        sns.scatterplot(data=X_train, x=X_train[:,0], y=X_train[:,1],ax=ax, hue=y_train)\n",
    "        ax.set_title('Train')\n",
    "    else:\n",
    "        sns.scatterplot(data=X_test, x=X_test[:,0], y=X_test[:,1],ax=ax, hue=y_test)\n",
    "        ax.set_title('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should implement *k-means* algorithm for the $L_2$ norm. `df` should containg the examples and thier assigned cluster. Notice that cluster numbers should range from 1:K and not from 0:K-1. Your implementation here is basically stage (4) of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1\n",
    "def k_means(mu,x_train):\n",
    "    K = mu.shape[0]\n",
    "    df = pd.DataFrame(columns=['x_i','C_i'])\n",
    "    n_iter = 10\n",
    "    with tqdm(total=n_iter, file=sys.stdout) as pbar:\n",
    "        for i in range(n_iter):\n",
    "            pbar.set_description('processed: %d/%d' % ((1 + i), n_iter))\n",
    "            pbar.update(1)\n",
    "            for idx_row in range(x_train.shape[0]):\n",
    "            #---------------------------------------------------------------------------\n",
    "\n",
    "                for idx, m in enumerate(mu):\n",
    "                #---------------------------------------------------------------------\n",
    "\n",
    "                #--------------------------------------------------------------------\n",
    "\n",
    "            #--------------------------------------------------------------------------\n",
    "            mu = np.empty((0, 2))\n",
    "            for k in range(K):\n",
    "                mu = np.concatenate([mu, df['x_i'].loc[df.loc[:,'C_i']==k+1].values.mean()])\n",
    "    return mu, df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's intialize the centroids as follows ($K=2$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mu = np.array([[0 ,0], [-0.5, 1]])\n",
    "mu, cluster_df_train = k_means(init_mu,X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the clustering according to the learned centroids applied on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2\n",
    "def test_cluster(mu,x_test):\n",
    "    df = pd.DataFrame(columns=['x_i','C_i'])\n",
    "    K = mu.shape[0] \n",
    "    for idx_row in range(x_test.shape[0]):\n",
    "    #---------------------------------------------------------------------------\n",
    "\n",
    "        for idx, m in enumerate(mu):\n",
    "        #---------------------------------------------------------------------------\n",
    "\n",
    "        #---------------------------------------------------------------------------\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_test = test_cluster(mu,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Kmeans(X_train,X_test,cluster_df_train,cluster_df_test,mu,init_mu):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        if idx == 0:\n",
    "            sns.scatterplot(data=X_train, x=X_train[:,0], y=X_train[:,1],ax=ax, hue=cluster_df_train['C_i'])\n",
    "            ax.plot(mu[:,0],mu[:,1],'b+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "            ax.plot(init_mu[:,0],init_mu[:,1],'g+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "            ax.legend(('new','init'))\n",
    "            ax.set_title('Train')\n",
    "        else:\n",
    "            sns.scatterplot(data=X_test, x=X_test[:,0], y=X_test[:,1],ax=ax, hue=cluster_df_test['C_i'])\n",
    "            ax.plot(mu[:,0],mu[:,1],'b+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "            ax.plot(init_mu[:,0],init_mu[:,1],'g+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "            ax.legend(('new','init'))\n",
    "            ax.set_title('Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Kmeans(X_train,X_test,cluster_df_train,cluster_df_test,mu,init_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/1.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the initial centroids have updated. Even though it is not the \"correct\" number of clusters, the algorithm still converged and had its' results. This shows us the significance of choosing $K$. Let's try 5 clusters and visualize how the centroids are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3\n",
    "# Visualization of 5 clusters\n",
    "init_mu = np.array([[0 ,0], [-2, -1], [1.5,1], [1,1], [2,1]])\n",
    "mu = np.array([[0 ,0], [-2, -1], [1.5,1], [1,1], [2,1]])\n",
    "K = mu.shape[0]\n",
    "\n",
    "df = pd.DataFrame(columns=['x_i','C_i'])\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "n_iter = 6\n",
    "for i in range(n_iter):\n",
    "    for idx_row in range(X_train.shape[0]):\n",
    "    #---------------------------------------------------------------------------\n",
    "\n",
    "        for idx, m in enumerate(mu):\n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        #--------------------------------------------------------------------\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    ax.cla()\n",
    "    sns.scatterplot(data=X_train, x=X_train[:,0], y=X_train[:,1],ax=ax, hue=df['C_i'])\n",
    "    print(mu)\n",
    "    ax.plot(mu[:,0],mu[:,1],'b+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "    ax.plot(init_mu[:,0],init_mu[:,1],'g+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "    ax.set_title('Train')\n",
    "    display(fig)\n",
    "    clear_output(wait = True)\n",
    "    plt.pause(1.2)\n",
    "    mu = np.empty((0, 2))\n",
    "    for k in range(K):\n",
    "        mu = np.concatenate([mu, df['x_i'].loc[df.loc[:,'C_i']==k+1].values.mean()])\n",
    "ax.legend(('new','init'))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/2.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the effect of intialization, try the following centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4\n",
    "# Visualization of 5 clusters with different initial weights\n",
    "init_mu = np.array([[-2 ,2], [2, 2], [1.5,-1.5], [0,0], [0,-1]])\n",
    "mu = np.array([[-2 ,2], [2, 2], [1.5,-1.5], [0,0], [0,-1]])\n",
    "K = mu.shape[0]\n",
    "df = pd.DataFrame(columns=['x_i','C_i'])\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "n_iter = 10\n",
    "for i in range(n_iter):\n",
    "    for idx_row in range(X_train.shape[0]):\n",
    "    #---------------------------------------------------------------------------\n",
    "\n",
    "        for idx, m in enumerate(mu):\n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        #--------------------------------------------------------------------\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    ax.cla()\n",
    "    sns.scatterplot(data=X_train, x=X_train[:,0], y=X_train[:,1],ax=ax, hue=df['C_i'])\n",
    "    ax.plot(mu[:,0],mu[:,1],'b+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "    ax.plot(init_mu[:,0],init_mu[:,1],'g+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "    ax.set_title('Train')\n",
    "    display(fig)\n",
    "    clear_output(wait = True)\n",
    "    plt.pause(1.2)\n",
    "    mu = np.empty((0, 2))\n",
    "    for k in range(K):\n",
    "        mu = np.concatenate([mu, df['x_i'].loc[df.loc[:,'C_i']==k+1].values.mean()])\n",
    "ax.legend(('new','init'))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/3.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\">***Question:***</span> *Did you get any different results?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` has of course its' own implemetation. Notice that the k-means algorithm is very fast but tends to falls in local minima. Thus, it can be useful to restart it several times. We do it with `n_init` key-value argument. In addition, we can pick \"smart\" initializations using `init:k-means++`. Build a dictionary with smart initialization, 10 \"runs\", random state of 336546 and maximum iterations of 10. Then use a for loop that runs $K$ from 1 to 10 and saves the SSE for every cluseter (notice `inertia_` attribute in `KMeans`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5\n",
    "sse = []\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the results and look for an \"elbow\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\">***Question:***</span> *Where do you expect to have the elbow at?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/4.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly (or not), the elbow is found at $K=3$. Sometimes, it would be a bit difficult to see that so we can calculate it using `KneeLoacator` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "kl.elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the silhouette_coefficients using silhouette_score (that was already imported) as a function of number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C6\n",
    "silhouette_coefficients = []\n",
    "# Notice you start at 2 clusters for silhouette coefficient\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 11), silhouette_coefficients)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Silhouette coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "<center><img src=\"outputs/5.PNG\" width=\"480\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the \"correct\" number is 4. This shows us how inconclusive the results can be. Here are some more methods and attributes that you can apply with `KMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=4, n_init=10, max_iter=10, random_state=336546)\n",
    "kmeans.fit(X_train)  # predict labels \n",
    "ax.plot(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],'g+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "sns.scatterplot(data=X_train, x=X_train[:,0], y=X_train[:,1], hue=kmeans.predict(X_train), ax=ax)\n",
    "\n",
    "print('SSE value is {:.2f}'.format(kmeans.inertia_))\n",
    "print('Clu×“ter centers are: {}'.format(kmeans.cluster_centers_))\n",
    "print('Number of iterations needed for convergence is: {}'.format(kmeans.n_iter_))\n",
    "print(kmeans.cluster_centers_[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare GMM and k-means. Below, you have a function that plots Gaussian contours (based on elipsoid's principle axes) and k-means centroids. Use`pred_c_kmeans` and `pred_c_GMM` for predictions of the algorithms. Compare the results for $K=2,3,4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7\n",
    "K = 4\n",
    "# k-means\n",
    "kmeans = KMeans(init=\"k-means++\",n_clusters=K,n_init=10,max_iter=10,random_state=336546)\n",
    "kmeans.fit(X_train)# predict labels \n",
    "pred_c_kmeans = kmeans.predict(X_train)\n",
    "# GMM\n",
    "GMM = GaussianMixture(n_components=K)\n",
    "GMM.fit(X_train)\n",
    "pred_c_GMM = GMM.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_iter = itertools.cycle(['r', 'g', 'b', 'y'])\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14,8))\n",
    "axs = axs.flatten()\n",
    "for i, (mean, covar, color) in enumerate(zip(\n",
    "        GMM.means_, GMM.covariances_, color_iter)):\n",
    "    v, w = np.linalg.eigh(covar)\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    u = w[0] / np.linalg.norm(w[0])\n",
    "    # as the DP will not use every component it has access to\n",
    "    # unless it needs it, we shouldn't plot the redundant\n",
    "    # components.\n",
    "    if not np.any(pred_c_GMM == i):\n",
    "        continue\n",
    "    axs[0].scatter(X_train[pred_c_GMM == i, 0], X_train[pred_c_GMM == i, 1], 7, color=color)\n",
    "    axs[0].plot(GMM.means_[:,0],GMM.means_[:,1],'k+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "\n",
    "    # Plot an ellipse to show the Gaussian component\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_alpha(0.5)\n",
    "    axs[0].add_artist(ell)\n",
    "\n",
    "sns.scatterplot(data=X_train, x=X_train[:,0], y=X_train[:,1], hue=pred_c_kmeans, ax=axs[1])\n",
    "axs[1].plot(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],'b+', linewidth=2, markersize=20,markeredgewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *This tutorial was written by [Moran Davoodi](mailto:morandavoodi@gmail.com) with the assitance of [Yuval Ben Sason](mailto:yuvalbse@gmail.com) & Kevin Kotzen*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
